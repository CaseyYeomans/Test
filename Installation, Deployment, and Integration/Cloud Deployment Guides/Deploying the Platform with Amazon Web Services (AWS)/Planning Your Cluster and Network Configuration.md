# Planning Your Cluster and Network Configuration

Deploying your EKS cluster involves establish the following properties in your **cluster.yaml** and **Settings.yaml.** Profisee provides templates for both in its [AWS GitHub Site](https://github.com/Profisee/kubernetes/tree/master/AWS-EKS-CLI).

- **cluster.yaml** defines the cluster name, region, and the quantity and size profile of the various nodes allocated in the cluster.
- **Settings.yaml**, as it applies to the cluster and networking, defines...
- the initial memory and CPU allocation and quota limits of each pod within the cluster for each node type in the cluster and...
- the external DNS name and address that will be used to access the platform from outside the AWS VPC.

The figure below outlines the basic requirements and data gathering steps needed to prepare for you cluster and network configuration.

![](https://Profisee.magentrixcloud.com/sys/staticasset/read/file-ic4439be1035a6f36.png)

### Determining the Profisee Container Version

The version of the Profisee Platform you deploy to your cluster is a function of the container version specified in **Settings.yaml**. In addition, you need to decide what version of the software you want to use because this also may affect the license generated by Profisee Support. See also [Obtaining Your Profisee License Azure Container Registry Credentials](https://support.profisee.com/wikis/profiseeplatform/obtaining_your_profisee_license_azure_container_registry_credentials).

### Preparing Your DNS Settings

The external DNS address of the Profisee Platform must be added to your corporate DNS infrastructure as part of the EKS cluster provisioning process for users and external services to access Profisee based on the DNS name you and your team assign for that Profisee instance. Once you have selected the name for the instance, you must:

1. Set the **externalDnsUrl** and **externalDnsName** properties in the **Settings.yaml** to the desired name. The **DNS URL** is merely an extension of the name prefixed by **https://**. For example, if the name is **mymdmsite.myco.com**, then the DNS URL is **<https://mymdmsite.myco.com>**. The Profisee license is encoded with this URL so it is important that the correct license associated with this instance be copied into the **licenseFileData** setting in Settings.yaml. Refer to [Obtaining Your Profisee License Azure Container Registry Credentials](https://support.profisee.com/wikis/profiseeplatform/obtaining_your_profisee_license_azure_container_registry_credentials) for more details.

![](https://Profisee.magentrixcloud.com/sys/staticasset/read/file-idd2fb2a974b2fb07.png)
2. Add the specified DNS name to your corporate DNS server. You will need to create a [CNAME](https://en.wikipedia.org/wiki/CNAME_record) (i.e. alias) that refers to the **DNS name** of the load balancer of your provisioned EKS Cluster. The figure below shows the AWS [A record](https://support.dnsimple.com/articles/a-record/) for the provisioned site. The CNAME created in your DNS of choice maps your external DNS name to the A record name of the EKS load balancer.
![](https://Profisee.magentrixcloud.com/sys/staticasset/read/file-i328d52e54ef20319.png)

There is a *chicken-and-egg paradox* that must be dealt with above. You must define your external DNS name and update your DNS server **before** the EKS cluster is provisioned. But you are not able to know the A record of the load balancer until **after** the EKS cluster has provisioned it. However, it is harmless to provide a temporary (i.e. fictitious) A record when first creating your DNS entry. You may then return to your DNS server after the EKS cluster has been provisioned and update the CNAME to the (now known) A record. Once your CNAME has been corrected, users referring to it will successfully be redirected to the proper server in your cluster.

### Establishing Your EKS Cluster & Node Group Requirements

Before proceeding to sizing considerations, you must give your EKS cluster a **name** and decide on the **region** to which it is assigned.

The figure below shows a snippet from the **cluster.yaml** file template that is used to provision the EKS cluster.

![](https://Profisee.magentrixcloud.com/sys/staticasset/read/file-i1f90c1a00cf0527a.png)

**IMPORTANT**: Your EKS cluster **must** be in the same region as your EBS volume. Take note of the decisions made in [Planning Your File Storage Configuration](https://support.profisee.com/wikis/profiseeplatform/planning_your_file_storage_configuration_aws) when specifying your cluster's region.

Understanding your Profisee cluster node group requirements involve answering the following question:

- Do we need a multi-node, load balanced Profisee cluster to provide high availability to service requests and to scale the web load or other processing loads to additional web servers in the cluster?
- What do we anticipate the workloads to be for various Profisee nodes (aka **pods**) within the cluster?
- How many users will the instance serve?
- Are there large matching and deduplication workloads expected?
- What load will external 3rd-party applications place on the server?
- Are there any specialized node groups required?
- Is this instance intended for production or development/test workloads?
- What are the cost tradeoffs between running a more powerful node with multiple pods or having a dedicated node for each pod in the cluster?

The answer to these questions will determine the structure of your cluster in terms of the number of node groups and nodes and the scalability and power in terms of the [EC2 Instance Types](https://aws.amazon.com/ec2/instance-types/?trkCampaign=acq_paid_search_brand&sc_channel=PS&sc_campaign=acquisition_US&sc_publisher=Google&sc_category=Cloud%20Computing&sc_country=US&sc_geo=NAMER&sc_outcome=acq&sc_detail=ec2%20instance%20types&sc_content=%7bad%20group%7d&sc_matchtype=e&sc_segment=488982705765&sc_medium=ACQ-P|PS-GO|Brand|Desktop|SU|Cloud%20Computing|EC2|US|EN|Sitelink&s_kwcid=AL!4422!3!488982705765!e!!g!!ec2%20instance%20types&ef_id=CjwKCAjwpKCDBhBPEiwAFgBzj3G-mLNArhJYIoayN8HyZ4EySVxFcCoTBovKskva8YMXiDmNMRwHkhoCvEAQAvD_BwE:G:s&s_kwcid=AL!4422!3!488982705765!e!!g!!ec2%20instance%20types) for each of the node groups specified for the cluster. When considering the choice of instance types, recall that there are two node group classifications in a Profisee EKS cluster:

1. The **Linux** node group that provides cluster coordination and management. The Linux node(s) in a Profisee cluster require little scale and horsepower as they are not part of critical MDM workloads. They exist merely to manage the cluster and keep the lights on.
2. The **Windows** node group(s) contain all the nodes responsible for running the Profisee container pods. Depending on the number of nodes in the cluster and whether there is any specialization required for any of those nodes, you may need to create additional node groups that use a different EC2 instance types.

By default, Profisee's template sets the Linux node group's **instanceType** to [t2.large](https://aws.amazon.com/ec2/instance-types/t2/) which is adequate for most usage scenarios and can be lowered for non-production Profisee instances.

The instance type selection for the Windows node group requires the most consideration when determining size and scale needs. Profisee's template defaults the node group's **instanceType** to [m5.xlarge](https://aws.amazon.com/ec2/instance-types/m5/) and sets the **volumeSize** to 100 GB. These settings are a good starting point for average workloads where matching is not involved.

![](https://Profisee.magentrixcloud.com/sys/staticasset/read/file-i6be64c8f92f48073.png)

The **minSize** value determines the minimum number of nodes to allocate when the cluster is started. Profisee defaults this to 1 in the **cluster.yaml** template. However, there are additional considerations that will drive your Windows node group configuration:

- Do you need a multi-node, load balanced Profisee cluster? And if yes...
- Do you have any specialization needs related to any of the Profisee cluster nodes (aka EKS pods)? Do any of the node groups need more power or memory?

In a large production server, you may want to scale and load balance your web requests over several web and application servers. If this is the case, then you will need to adjust the **minSize** to account for the number of Profisee web/application servers you plan to configure using **Cluster Servers** administration in FastApp Studio.

Additionally, you may also need to create multiple Windows node groups if you need the nodes in one group use a different EC2 instance type than other node groups. The typical reason for this is if you plan to establish a **matching processing** server within your Profisee cluster. Matching can be a heavy workload and memory intensive process depending on the size of the associated entity and the number and complexity of the matching attributes. You will only want to incur the cost of additional CPU and memory for those Profisee cluster nodes (EKS pods) that need it for matching. The figure below illustrates how a 3-Profisee node (pod) cluster would be configured for the following requirements:

- 2 load balanced web application servers with 4 vCPU and 16 GB memory
- 1 matching server with 32 vCPU and 128 GB of memory

![](https://Profisee.magentrixcloud.com/sys/staticasset/read/file-id2475bad06f7dbb4.png)

Note how the configuration has changed above. Each Windows node group...

- is given a distinct name,
- uses a different **instanceType**,
- has a **minSize** relative to the number of nodes of that type needed in the Profisee cluster.

If you only need a pair of load balanced web/application servers, the same configuration could be used but you could eliminate **windows-ng2**.

Minimal requirements for a Profisee web/application server include:

- 8 cores
- 8 GB RAM

Recommended specifications for your application servers are subject to the considerations specified above. Additional scaling and workload guidance can be found in the ***System Requirements for Profisee Server*** section of the [Profisee Platform Installation Guide](https://profisee.magentrixcloud.com/wikis/profiseeplatform/system_requirements_for_profisee_server) on the [Profisee Customer Portal](https://profisee.magentrixcloud.com/aspx/CustomerHome).

If you are a prospect considering Profisee and do not yet have a login to the Profisee Customer Portal, work with your account executive to obtain a PDF copy of this document for your consideration and planning purposes.

You should set the availability zone (AZ) to an AZ that exists within the region you specified for your cluster.

EKS can spread worker nodes across multiple AZs. Such scaling and high availability strategies are beyond the scope of this document. You should refer to detailed documentation with AWS including but not limited to [Creating Kubernetes Auto Scaling Groups for Multiple Availability Zones](https://aws.amazon.com/blogs/containers/amazon-eks-cluster-multi-zone-auto-scaling-groups/) for guidance on complex scaling and failover configurations.

### Setting Your Cluster Node Initial & Maximum Allocation

Kubernetes will auto-scale resources as needed for the nodes in your EKS cluster. The configuration allows you to control how much memory and the number of vCPUs pods are allocated at startup and their maximum limits. These settings can be found in the **clusterNode** section of the **Settings.yaml** file as shown in the snippet below:

![](https://Profisee.magentrixcloud.com/sys/staticasset/read/file-ibed20b09dee8ef21.png)

The **requests** section indicates the starting allocation and the growth increments when Kubernetes needs to scale the pod up while the limits section indicates the maximum allocation of the assigned **instanceType** from the **cluster.yaml** file specified for the node group. The Profisee defaults specified above assume that each pod (container) gets a dedicated node. Therefore, the cpu and memory of 1000 and 10T respectively just tell Kubernetes to give all the resources (memory and CPU) to the pod running on that node up to the maximum available based on the **instanceType** specified for the node group.

The configuration specified in the sample above can be used as-is in most case. The only time this would need to be adjusted would be if you and your team decided that you wanted to use a single EKS node (i.e. virtual machine) to host multiple pods (i.e. Profisee containers). In this case, you would need to be cognizant of the limits (vCPU and memory) of the associated **instanceType** specified for the node group. For example, if you were to host 2 web/application servers on a single node of instance type m5.4xlarge (16 vCPUs and 64 GB memory) you would want to evenly distribute the resources for the pods as shown below:

![](https://Profisee.magentrixcloud.com/sys/staticasset/read/file-i85eaccde8b2fc531.png)

This configuration tells Kubernetes to allocate 2 vCPUs and 8 GB of RAM to each pod running on the node. As resource needs increase, Kubernetes will allocate additional vCPUs and memory at increments of 2 and 8 GB respectively until each pod reaches its maximum limit of 8 vCPUs and 32 GB, respectively.